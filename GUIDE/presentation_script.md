# 🎤 PPT 발표 스크립트: 5-Layer 실시간 데이터 파이프라인

---

## Layer 1 & 2: Initialization & Data Generator

### 📌 슬라이드 제목
**"데이터 생성의 시작점 - 단순 랜덤이 아닌 규칙 기반 트래픽 생성"**

---

### 🏗️ 구조
```
┌─────────────────────────────────────────────────────────────────┐
│  Layer 1: Initialization          Layer 2: Data Generator       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌──────────────┐                  ┌────────────────────────┐  │
│   │ User Seeder  │──┐               │      Redis             │  │
│   │ (고객 생성)    │  │    50초마다   │   ┌────────────────┐  │  │
│   └──────────────┘  ├──────────────▶│   │ 고객 1000명     │  │  │
│                     │               │   │ 상품 1000개     │  │  │
│   ┌──────────────┐  │               │   └────────────────┘  │  │
│   │ Scenario     │──┘               │          │            │  │
│   │ Producer     │                  │          ▼            │  │
│   │ (시나리오)     │                  │   ┌────────────────┐  │  │
│   └──────────────┘                  │   │ Cache Worker   │  │  │
│                                     │   │ (분리 적재)     │  │  │
│                                     │   └────────────────┘  │  │
│                                     └────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

---

### 📝 발표 내용

#### 1️⃣ Layer 1: Initialization (초기화 계층)

**무엇을 하는가?**
> "시스템이 시작될 때 필요한 기초 데이터를 생성합니다."

| 컴포넌트 | 역할 | 상세 |
|----------|------|------|
| **User Seeder** | 초기 고객 생성 | 1만 명의 고객 데이터 생성 (이름, 나이, 성별, 가입일 등) |
| **Initial Seeder** | 초기 상품 생성 | 2만 개의 상품 데이터 생성 (카테고리, 가격, 재고 등) |

**핵심 포인트:**
- 모든 신규 고객은 **BRONZE 등급**으로 시작
- 상품 가격은 **베타 분포**를 사용하여 현실적인 가격대 분포 생성

---

#### 2️⃣ Layer 2: Data Generator (데이터 생성 계층) ⭐핵심

**무엇을 하는가?**
> "실시간으로 주문/상품/고객을 생성하는데, **무작위가 아닌 규칙 기반**으로 생성합니다."

---

##### 🔴 Redis를 사용하는 이유

| 문제 | 해결책 |
|------|--------|
| 매 주문마다 DB 조회하면 부하 폭증 | Redis에 1,000명만 캐싱해서 메모리에서 빠르게 조회 |
| DB 쿼리 속도 느림 | Redis는 메모리 기반이라 **100배 빠름** |
| 동시 접속 시 DB 병목 | Redis가 **버퍼 역할**을 해서 DB 부하 분산 |

**결과:** DB 쿼리 98% 감소, 조회 속도 100배 향상

---

##### 🔴 Cache Worker가 하는 일

50초마다 다음 작업을 수행:

```
PostgreSQL에서 1,000명 고객을 선별하여 Redis에 적재

┌─────────────────────────────────────────────┐
│          고객 1,000명 분리 적재               │
├─────────────────────────────────────────────┤
│  🔵 구매이력 고객 600명                       │
│     → 마지막 주문일이 오래된 순 (재구매 유도)   │
│                                             │
│  🟢 미구매 고객 400명                         │
│     → 최근 가입한 순 (첫 구매 유도)            │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│          상품 1,000개 분리 적재               │
├─────────────────────────────────────────────┤
│  🔵 인기 상품 700개                          │
│     → 주문 수 많은 순 (베스트셀러)             │
│                                             │
│  🟢 신상품 300개                             │
│     → 최근 등록된 순 (신상품 노출)             │
└─────────────────────────────────────────────┘
```

**왜 이렇게 나누는가?**
- 실제 쇼핑몰처럼 **재구매 고객 + 신규 고객** 혼합
- 실제 쇼핑몰처럼 **베스트셀러 + 신상품** 혼합
- 더 현실적인 구매 패턴 생성 가능

---

##### 🔴 구매 성향 점수 시스템 (핵심 알고리즘)

**"누가 지금 구매할 확률이 높은가?"를 점수로 계산**

```
최종 점수 = 기본 점수 × 시간대 변동 × 마케팅 부스트 × 생활 이벤트
```

| 요소 | 설명 | 예시 |
|------|------|------|
| **기본 점수** | 나이 + 성별 + 상태 + 마케팅동의 + 등급 | 30대 여성 VIP = 높은 점수 |
| **시간대 변동** | 연령대별 피크 시간 반영 | 20대는 밤 9시, 50대는 오전 10시에 피크 |
| **마케팅 부스트** | 마케팅 동의자 20%에게 1.3배 | 프로모션 알림 받은 사람 |
| **생활 이벤트** | 랜덤 소비 충동 (월급날, 스트레스 등) | 85% 보통 / 12% 소비증가 / 3% 대규모지출 |

**결과:** 점수가 높은 상위 N명만 실제 주문 생성 → 현실적인 구매 패턴!

---

##### 🔴 시나리오 엔진 (20가지 이벤트)

| 시나리오 | 영향 |
|----------|------|
| 설날/추석 | 선물 카테고리 주문 급증 |
| 블랙프라이데이 | 전체 주문량 3배 증가 |
| 여름휴가 시즌 | 여행/레저 상품 증가 |
| 월초/월말 | 월급날 효과로 주문 증가 |

---

##### 🔴 데이터 생성 주기

| 데이터 | 생성 간격 | 특징 |
|--------|----------|------|
| **주문** | 3~5초 | 장바구니 (1~10개 상품 동시 구매) |
| **상품** | 6~8초 | 신상품 등록 |
| **고객** | S커브 감쇄 | 초기 10명/10초 → 점진적 감소 (서비스 성장 패턴 모방) |

---

---

## Layer 3: Processing (Kafka & Consumer)

### 📌 슬라이드 제목
**"실시간 스트림 처리 - 대용량 데이터의 안정적인 전달"**

---

### 🏗️ 구조
```
┌───────────────────────────────────────────────────────────────┐
│                    Layer 3: Processing                         │
├───────────────────────────────────────────────────────────────┤
│                                                               │
│   ┌──────────────┐      ┌─────────────────────────────────┐   │
│   │   Producer   │      │      Kafka Cluster              │   │
│   │              │──────│     ┌─────────────────────┐     │   │
│   │ - 주문 발행   │      │     │    3 Brokers        │     │   │
│   │ - 상품 발행   │      │     │                     │     │   │
│   │ - 고객 발행   │      │     │  Topic: users       │     │   │
│   └──────────────┘      │     │  Topic: products    │     │   │
│                         │     │  Topic: orders      │     │   │
│                         │     └─────────────────────┘     │   │
│                         └───────────────┬─────────────────┘   │
│                                         │                     │
│                                         ▼                     │
│   ┌─────────────────────────────────────────────────────────┐ │
│   │                  9개 Consumer Group                      │ │
│   ├─────────────────────────────────────────────────────────┤ │
│   │  User Consumer ×3 │ Product Consumer ×3 │ Order Consumer ×3│
│   └─────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────┘
```

---

### 📝 발표 내용

#### 🔴 Kafka를 사용하는 이유

| 문제 | Kafka 해결책 |
|------|-------------|
| 데이터 유실 위험 | 메시지를 **디스크에 저장** → 장애 시에도 복구 가능 |
| 단일 서버 병목 | **3개 브로커** 클러스터로 부하 분산 |
| Consumer 느림 문제 | Producer와 Consumer **비동기 분리** → 서로 영향 없음 |
| 확장성 | 나중에 Consumer만 추가하면 처리량 증가 |

---

#### 🔴 Producer의 역할

```
Layer 2에서 생성된 데이터를 Kafka 토픽으로 발행

┌─────────────────────────────────────────────┐
│              Producer 발행                   │
├─────────────────────────────────────────────┤
│  주문 데이터 ──────▶ orders 토픽             │
│  상품 데이터 ──────▶ products 토픽           │
│  고객 데이터 ──────▶ users 토픽              │
└─────────────────────────────────────────────┘
```

---

#### 🔴 Consumer Group 구조 (9개)

**왜 3개씩?** → **병렬 처리로 처리 속도 3배 향상**

| Consumer Group | 인스턴스 | 담당 토픽 |
|----------------|----------|----------|
| User Consumer | 3개 | users 토픽 |
| Product Consumer | 3개 | products 토픽 |
| Order Consumer | 3개 | orders 토픽 |

**작동 방식:**
1. Kafka가 메시지를 **파티션별로 분배**
2. 같은 그룹 내 Consumer끼리 **메시지 중복 없이 분담**
3. 한 Consumer가 죽어도 다른 Consumer가 **자동 인수**

---

#### 🔴 데이터 흐름 예시

```
1. Producer가 주문 100건 발행
   ↓
2. Kafka가 3개 파티션에 분배 (33건씩)
   ↓
3. Order Consumer 3개가 각각 33건씩 병렬 처리
   ↓
4. PostgreSQL에 저장
```

---

---

## Layer 4: Analytics & Storage

### 📌 슬라이드 제목
**"데이터의 영속화와 분석 - PostgreSQL + Spark Streaming"**

---

### 🏗️ 구조
```
┌───────────────────────────────────────────────────────────────┐
│                    Layer 4: Analytics & Storage                │
├───────────────────────────────────────────────────────────────┤
│                                                               │
│   ┌─────────────────────────────────────────────────────────┐ │
│   │                    PostgreSQL                            │ │
│   ├─────────────────────────────────────────────────────────┤ │
│   │  📦 users 테이블      │  고객 정보 + 등급                 │ │
│   │  📦 products 테이블   │  상품 정보 + 주문수               │ │
│   │  📦 orders 테이블     │  주문 정보 + 상품 목록            │ │
│   └─────────────────────────────────────────────────────────┘ │
│                                                               │
│   ┌─────────────────────────────────────────────────────────┐ │
│   │                  Spark Streaming                         │ │
│   ├─────────────────────────────────────────────────────────┤ │
│   │  📊 실시간 집계  │  📈 트렌드 분석  │  🔄 배치 처리        │ │
│   └─────────────────────────────────────────────────────────┘ │
│                                                               │
│   ┌─────────────────────────────────────────────────────────┐ │
│   │                  Grade Updater (등급 갱신)               │ │
│   ├─────────────────────────────────────────────────────────┤ │
│   │  10분마다 6개월 누적 구매금액/횟수 기준 등급 자동 갱신     │ │
│   └─────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────┘
```

---

### 📝 발표 내용

#### 🔴 PostgreSQL의 역할

| 기능 | 설명 |
|------|------|
| **영구 저장소** | Kafka에서 소비된 모든 데이터의 최종 저장소 |
| **ACID 보장** | 트랜잭션으로 데이터 일관성 보장 |
| **관계형 쿼리** | JOIN으로 고객-주문-상품 연결 분석 가능 |

---

#### 🔴 Spark Streaming을 사용하는 이유

| 문제 | Spark 해결책 |
|------|-------------|
| 대용량 데이터 집계 느림 | **분산 처리**로 빠른 집계 |
| 실시간 분석 필요 | **스트리밍 모드**로 실시간 처리 |
| 복잡한 분석 쿼리 | **SQL-like API**로 쉬운 분석 |

**Spark가 하는 일:**
- 시간대별 주문량 집계
- 카테고리별 매출 분석
- 고객 행동 패턴 분석
- Grafana로 데이터 전달

---

#### 🔴 등급 자동 갱신 시스템

**10분마다 배치로 실행**

| 등급 | 조건 (6개월 누적) |
|------|------------------|
| **VIP** | 500만원 이상 AND 30회 이상 |
| **GOLD** | 200만원 이상 AND 15회 이상 |
| **SILVER** | 50만원 이상 AND 5회 이상 |
| **BRONZE** | 조건 미달 (기본) |

**강등 정책:**
- 한 번에 **1단계씩만** 강등 (VIP→GOLD→SILVER→BRONZE)
- 급격한 등급 하락 방지 → 고객 이탈 방지

---

---

## Layer 5: Insight Visualization

### 📌 슬라이드 제목
**"데이터 시각화 - Grafana 대시보드로 인사이트 도출"**

---

### 🏗️ 구조
```
┌───────────────────────────────────────────────────────────────┐
│                 Layer 5: Insight Visualization                 │
├───────────────────────────────────────────────────────────────┤
│                                                               │
│   ┌─────────────────────────────────────────────────────────┐ │
│   │                  Grafana Dashboard                       │ │
│   ├─────────────────────────────────────────────────────────┤ │
│   │                                                         │ │
│   │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │ │
│   │  │ 실시간 현황  │  │ 트렌드 분석  │  │ 고객 분석   │     │ │
│   │  │             │  │             │  │             │     │ │
│   │  │ - 주문/분    │  │ - 월별 매출  │  │ - 등급 분포  │     │ │
│   │  │ - 활성사용자 │  │ - 성장률    │  │ - 재구매율  │     │ │
│   │  │ - Kafka상태 │  │ - 시즌 패턴  │  │ - LTV      │     │ │
│   │  └─────────────┘  └─────────────┘  └─────────────┘     │ │
│   │                                                         │ │
│   │  ┌─────────────────────────────────────────────────┐   │ │
│   │  │              알림 및 모니터링                      │   │ │
│   │  │  ⚠️ 주문량 급감 알림  │  ⚠️ 시스템 장애 알림        │   │ │
│   │  └─────────────────────────────────────────────────┘   │ │
│   └─────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────┘
```

---

### 📝 발표 내용

#### 🔴 Grafana를 사용하는 이유

| 장점 | 설명 |
|------|------|
| **실시간 시각화** | 데이터 변화를 즉시 그래프로 확인 |
| **다양한 데이터소스** | PostgreSQL, Spark 등 다양한 소스 연결 |
| **커스텀 대시보드** | 원하는 지표만 골라서 구성 |
| **알림 기능** | 이상 징후 발생 시 자동 알림 |

---

#### 🔴 주요 대시보드 패널

| 패널 | 시각화 내용 |
|------|------------|
| **실시간 주문 현황** | 분당 주문 수, 매출액 추이 |
| **고객 등급 분포** | VIP/GOLD/SILVER/BRONZE 비율 파이차트 |
| **시간대별 트래픽** | 24시간 주문량 히트맵 |
| **카테고리별 매출** | 상품 카테고리별 막대 그래프 |
| **월별 성장 추이** | 1년간 매출/주문 성장 라인 차트 |

---

#### 🔴 비즈니스 인사이트 예시

```
📊 대시보드에서 발견할 수 있는 인사이트:

1. "20대 여성 고객이 밤 9시에 가장 많이 구매"
   → 해당 시간대 타겟 마케팅

2. "VIP 고객의 재구매 주기는 평균 14일"
   → 13일째 리마인더 쿠폰 발송

3. "월초에 주문량 30% 증가"
   → 월급날 프로모션 집중

4. "신상품이 등록 후 3일 내 첫 구매 발생"
   → 신상품 노출 전략 수립
```

---

---

## 🎯 발표 마무리 멘트

> **"이 5-Layer 파이프라인은 단순히 데이터를 생성하고 저장하는 것이 아닙니다.**
> 
> **Layer 2의 구매 성향 시스템**으로 현실적인 데이터를 만들고,  
> **Layer 3의 Kafka**로 안정적으로 전달하며,  
> **Layer 4의 PostgreSQL + Spark**로 분석하고,  
> **Layer 5의 Grafana**로 비즈니스 인사이트를 도출합니다.
> 
> 결과적으로, **실제 이커머스 서비스와 동일한 데이터 패턴**을 가진 분석 환경을 구축했습니다."
