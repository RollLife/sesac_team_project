version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: local_postgres
    restart: always
    environment:
      DB_TYPE: local
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: ${POSTGRES_DB:-sesac_db}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Kafka Broker 1 (Leader)
  kafka1:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka1
    restart: always
    environment:
      # KRaft ì„¤ì •
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19093,2@kafka2:19093,3@kafka3:19093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # ë¦¬ìŠ¤ë„ˆ ì„¤ì •
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:19093,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka1:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL

      # í´ëŸ¬ìŠ¤í„° ì„¤ì • (3ê°œ ë¸Œë¡œì»¤, ë³µì œ íŒ©í„° 3)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'  # ìˆ˜ë™ í† í”½ ìƒì„±
      KAFKA_MIN_INSYNC_REPLICAS: 2

      # KRaft ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    ports:
      - "9092:9092"
    volumes:
      - kafka1_data:/var/lib/kafka/data
    command: >
      bash -c "
      if [ ! -f /var/lib/kafka/data/meta.properties ]; then
        kafka-storage format -t MkU3OEVBNTcwNTJENDM2Qk -c /etc/kafka/kafka.properties;
      fi;
      /etc/confluent/docker/run
      "

  # Kafka Broker 2 (Replica)
  kafka2:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka2
    restart: always
    environment:
      # KRaft ì„¤ì •
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19093,2@kafka2:19093,3@kafka3:19093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # ë¦¬ìŠ¤ë„ˆ ì„¤ì •
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093,CONTROLLER://0.0.0.0:19093,PLAINTEXT_INTERNAL://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9093,PLAINTEXT_INTERNAL://kafka2:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL

      # í´ëŸ¬ìŠ¤í„° ì„¤ì •
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_MIN_INSYNC_REPLICAS: 2

      # KRaft ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    ports:
      - "9093:9093"
    volumes:
      - kafka2_data:/var/lib/kafka/data
    command: >
      bash -c "
      if [ ! -f /var/lib/kafka/data/meta.properties ]; then
        kafka-storage format -t MkU3OEVBNTcwNTJENDM2Qk -c /etc/kafka/kafka.properties;
      fi;
      /etc/confluent/docker/run
      "

  # Kafka Broker 3 (Replica)
  kafka3:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka3
    restart: always
    environment:
      # KRaft ì„¤ì •
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19093,2@kafka2:19093,3@kafka3:19093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # ë¦¬ìŠ¤ë„ˆ ì„¤ì •
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,CONTROLLER://0.0.0.0:19093,PLAINTEXT_INTERNAL://0.0.0.0:29094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,PLAINTEXT_INTERNAL://kafka3:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL

      # í´ëŸ¬ìŠ¤í„° ì„¤ì •
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_MIN_INSYNC_REPLICAS: 2

      # KRaft ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    ports:
      - "9094:9094"
    volumes:
      - kafka3_data:/var/lib/kafka/data
    command: >
      bash -c "
      if [ ! -f /var/lib/kafka/data/meta.properties ]; then
        kafka-storage format -t MkU3OEVBNTcwNTJENDM2Qk -c /etc/kafka/kafka.properties;
      fi;
      /etc/confluent/docker/run
      "

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: local_kafka_ui
    restart: always
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    environment:
      KAFKA_CLUSTERS_0_NAME: local-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:29092,kafka2:29093,kafka3:29094
    ports:
      - "8080:8080"

  # PostgreSQL ê´€ë¦¬ ë„êµ¬
  adminer:
    image: adminer:latest
    container_name: local_adminer
    restart: always
    depends_on:
      - postgres
    ports:
      - "8081:8080"
    environment:
      ADMINER_DEFAULT_SERVER: postgres

  # Redis ìºì‹œ ì„œë²„ (Aging ê¸°ë²• ì ìš©)
  redis:
    image: redis:7-alpine
    container_name: local_redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ============================================
  # Python ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë¹„ìŠ¤
  # ============================================

  # DB í…Œì´ë¸” ì´ˆê¸°í™”
  db-init:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: db_init
    depends_on:
      - postgres
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
    command: >
      bash -c "
        echo 'â³ PostgreSQL ì¤€ë¹„ ëŒ€ê¸° ì¤‘...';
        sleep 5;
        echo 'ğŸš€ í…Œì´ë¸” ìƒì„± ì‹œì‘...';
        python -c 'from database.init_db import init_db; init_db()';
        echo 'âœ… í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ';
      "
    restart: "no"

  # Kafka í† í”½ ì´ˆê¸°í™” (ë¸Œë¡œì»¤ ì¤€ë¹„ ëŒ€ê¸° í›„ í† í”½ ìƒì„±)
  kafka-init:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: kafka_init
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: >
      bash -c "
        echo 'â³ Kafka ë¸Œë¡œì»¤ ì¤€ë¹„ ëŒ€ê¸° ì¤‘...';
        sleep 15;
        echo 'ğŸš€ í† í”½ ìƒì„± ì‹œì‘...';
        python kafka/admin/setup_topics.py;
        echo 'âœ… í† í”½ ì´ˆê¸°í™” ì™„ë£Œ';
      "
    restart: "no"

  # ì´ˆê¸° ë°ì´í„° ìƒì„± (one-time job)
  initial-seeder:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: initial_seeder
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python apps/seeders/initial_seeder.py
    restart: "no"  # í•œ ë²ˆë§Œ ì‹¤í–‰

  # Redis ìºì‹œ ì›Œì»¤ (Aging ê¸°ë²•ìœ¼ë¡œ DB â†’ Redis ê°±ì‹ )
  cache-worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: cache_worker
    depends_on:
      - db-init
      - redis
      - initial-seeder
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_ENABLED: "true"
      CACHE_REFRESH_INTERVAL: 50
      CACHE_BATCH_SIZE: 1000
      CACHE_NEW_DATA_RATIO: 0.5
    command: python cache/cache_worker.py
    restart: always

  # Redis ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
  redis-monitor:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: redis_monitor
    depends_on:
      - redis
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_MONITOR_INTERVAL: 1
      CACHE_REFRESH_INTERVAL: 50
    command: python cache/redis_monitor.py
    restart: always

  # ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„± (Producer)
  producer:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: realtime_producer
    depends_on:
      - db-init
      - kafka-init
      - initial-seeder
      - redis
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_ENABLED: "true"
    command: python apps/seeders/realtime_generator.py
    restart: always

  # ì‹¤ì‹œê°„ ê³ ê° ë°ì´í„° ìƒì„± (User Seeder)
  user-seeder:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: user_seeder
    depends_on:
      - db-init
      - kafka-init
      - initial-seeder
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python apps/seeders/realtime_generator_user.py --batch-size 10 --interval 10
    restart: always

  # Consumer - Users Group (3ê°œ ì¸ìŠ¤í„´ìŠ¤)
  user-consumer-1:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: user_consumer_1
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/user_consumer.py --id user_consumer_1
    restart: always

  user-consumer-2:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: user_consumer_2
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/user_consumer.py --id user_consumer_2
    restart: always

  user-consumer-3:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: user_consumer_3
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/user_consumer.py --id user_consumer_3
    restart: always

  # Consumer - Products Group (3ê°œ ì¸ìŠ¤í„´ìŠ¤)
  product-consumer-1:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: product_consumer_1
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/product_consumer.py --id product_consumer_1
    restart: always

  product-consumer-2:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: product_consumer_2
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/product_consumer.py --id product_consumer_2
    restart: always

  product-consumer-3:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: product_consumer_3
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/product_consumer.py --id product_consumer_3
    restart: always

  # Consumer - Orders Group (3ê°œ ì¸ìŠ¤í„´ìŠ¤)
  order-consumer-1:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: order_consumer_1
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/order_consumer.py --id order_consumer_1
    restart: always

  order-consumer-2:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: order_consumer_2
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/order_consumer.py --id order_consumer_2
    restart: always
    
  # ============================================  
  # Spark Cluster (Analytics)
  # ============================================
  spark-master:
      image: apache/spark:3.5.0
      container_name: spark_master
      # [í•µì‹¬] ê³µì‹ ì´ë¯¸ì§€ëŠ” ì´ ëª…ë ¹ì–´ë¥¼ ì¤˜ì•¼ ë§ˆìŠ¤í„°ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
      command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      user: root
      ports:
        - "8082:8080" # Web UI
        - "7077:7077" # RPC Port
      volumes:
        - ..:/app
      # networks:
      #   - default # (ì‚¬ìš© ì¤‘ì¸ ë„¤íŠ¸ì›Œí¬ ì´ë¦„ì´ ìˆë‹¤ë©´ ë³€ê²½)

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark_worker
    # [í•µì‹¬] ì›Œì»¤ ì‹¤í–‰ ëª…ë ¹ì–´ë¥¼ ì§ì ‘ ëª…ì‹œí•˜ê³ , ë§ˆìŠ¤í„° ì£¼ì†Œë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ..:/app
    # networks:
    #   - default

  # ============================================
  # Jupyter Notebook (Interactive Debugging)
  # ============================================
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter_notebook
    user: root # ê¶Œí•œ ë¬¸ì œ ë°©ì§€
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
    ports:
      - "8888:8888" # ì›¹ ë¸Œë¼ìš°ì € ì ‘ì†ìš© í¬íŠ¸
    volumes:
      - ..:/home/jovyan/work # í”„ë¡œì íŠ¸ í´ë”ë¥¼ ë…¸íŠ¸ë¶ ì‘ì—… í´ë”ë¡œ ì—°ê²°
    networks:
      - default

  order-consumer-3:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: order_consumer_3
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
    command: python kafka/consumers/order_consumer.py --id order_consumer_3
    restart: always

  # ============================================
  # Python ê°œë°œ/í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ
  # ============================================
  python-dev:
    build:
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: python_dev
    depends_on:
      - db-init
      - kafka-init
    environment:
      DB_TYPE: local
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sesac_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_ENABLED: "true"
      KAFKA_TOPIC_USERS: users
      KAFKA_TOPIC_PRODUCTS: products
      KAFKA_TOPIC_ORDERS: orders
    command: tail -f /dev/null  # ì»¨í…Œì´ë„ˆë¥¼ ê³„ì† ì‹¤í–‰ ìƒíƒœë¡œ ìœ ì§€
    stdin_open: true  # STDIN ì—´ê¸°
    tty: true  # TTY í• ë‹¹
    volumes:
      - .:/app  # ì½”ë“œ ë™ê¸°í™” (ê°œë°œìš©)
    profiles:
      - dev  # í”„ë¡œíŒŒì¼ ì„¤ì • (ì„ íƒì  ì‹¤í–‰)

volumes:
  postgres_data:
  kafka1_data:
  kafka2_data:
  kafka3_data:
  redis_data:
