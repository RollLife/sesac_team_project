"""
Spark ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¹„êµ ë° ë¦¬í¬íŠ¸ ìƒì„±

PostgreSQLê³¼ Sparkì˜ ì§‘ê³„ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  HTML ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import glob
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from benchmark_common import generate_comparison_html, RESULTS_DIR


def find_latest_results():
    """ê°€ì¥ ìµœê·¼ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°"""
    sql_files = glob.glob(os.path.join(RESULTS_DIR, "spark_benchmark_sql_*.json"))
    spark_files = glob.glob(os.path.join(RESULTS_DIR, "spark_benchmark_spark_*.json"))
    
    latest_sql = max(sql_files, key=os.path.getctime) if sql_files else None
    latest_spark = max(spark_files, key=os.path.getctime) if spark_files else None
    
    return latest_sql, latest_spark


def compare_results():
    """ê²°ê³¼ ë¹„êµ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Spark vs PostgreSQL ì§‘ê³„ ì„±ëŠ¥ ë¹„êµ                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    sql_file, spark_file = find_latest_results()
    
    if not sql_file or not spark_file:
        print("âŒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   PostgreSQL ê²°ê³¼: {'ìˆìŒ' if sql_file else 'ì—†ìŒ'}")
        print(f"   Spark ê²°ê³¼: {'ìˆìŒ' if spark_file else 'ì—†ìŒ'}")
        return None
    
    # ê²°ê³¼ ë¡œë“œ
    with open(sql_file, 'r', encoding='utf-8') as f:
        sql_result = json.load(f)
    
    with open(spark_file, 'r', encoding='utf-8') as f:
        spark_result = json.load(f)
    
    # ë¹„êµ ì¶œë ¥
    print(f"ğŸ“Š PostgreSQL ê²°ê³¼: {os.path.basename(sql_file)}")
    print(f"ğŸ“Š Spark ê²°ê³¼: {os.path.basename(spark_file)}")
    print()
    
    # ì„±ëŠ¥ ë¹„êµ
    sql_qps = sql_result.get('records_per_second', 0)
    spark_qps = spark_result.get('records_per_second', 0)
    
    sql_duration = sql_result.get('duration_seconds', 0)
    spark_duration = spark_result.get('duration_seconds', 0)
    
    sql_latency = sql_result.get('avg_latency_ms', 0)
    spark_latency = spark_result.get('avg_latency_ms', 0)
    
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚       ì§€í‘œ         â”‚  PostgreSQL  â”‚    Spark     â”‚    ì°¨ì´     â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    # QPS ë¹„êµ
    qps_diff = ((spark_qps - sql_qps) / sql_qps * 100) if sql_qps > 0 else 0
    qps_indicator = "â†‘" if qps_diff > 0 else "â†“"
    print(f"â”‚ QPS (queries/sec)  â”‚ {sql_qps:>10.1f}   â”‚ {spark_qps:>10.1f}   â”‚ {qps_indicator}{abs(qps_diff):>6.1f}%    â”‚")
    
    # ì†Œìš”ì‹œê°„ ë¹„êµ
    duration_diff = ((sql_duration - spark_duration) / sql_duration * 100) if sql_duration > 0 else 0
    duration_indicator = "â†‘" if duration_diff > 0 else "â†“"
    print(f"â”‚ ì†Œìš”ì‹œê°„ (ì´ˆ)       â”‚ {sql_duration:>10.2f}   â”‚ {spark_duration:>10.2f}   â”‚ {duration_indicator}{abs(duration_diff):>6.1f}%    â”‚")
    
    # í‰ê·  ì§€ì—° ë¹„êµ
    latency_diff = ((sql_latency - spark_latency) / sql_latency * 100) if sql_latency > 0 else 0
    latency_indicator = "â†‘" if latency_diff > 0 else "â†“"
    print(f"â”‚ í‰ê·  ì§€ì—° (ms)      â”‚ {sql_latency:>10.2f}   â”‚ {spark_latency:>10.2f}   â”‚ {latency_indicator}{abs(latency_diff):>6.1f}%    â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ
    sql_stats = sql_result.get('extra_info', {}).get('query_stats', {})
    spark_stats = spark_result.get('extra_info', {}).get('query_stats', {})
    
    if sql_stats and spark_stats:
        print(f"\n{'='*60}")
        print("ì¿¼ë¦¬ë³„ ìƒì„¸ ë¹„êµ")
        print(f"{'='*60}")
        print(f"{'ì¿¼ë¦¬ëª…':<20} {'PostgreSQL':<12} {'Spark':<12} {'ì°¨ì´':<10}")
        print("-" * 60)
        
        for query_name in sql_stats:
            if query_name in spark_stats:
                sql_avg = sql_stats[query_name].get('avg_ms', 0)
                spark_avg = spark_stats[query_name].get('avg_ms', 0)
                diff = ((sql_avg - spark_avg) / sql_avg * 100) if sql_avg > 0 else 0
                indicator = "â†‘" if diff > 0 else "â†“"
                print(f"{query_name:<20} {sql_avg:>10.2f}ms {spark_avg:>10.2f}ms {indicator}{abs(diff):>6.1f}%")
    
    # ê²°ë¡ 
    print()
    if spark_qps > sql_qps:
        improvement = (spark_qps / sql_qps - 1) * 100
        print(f"âœ… ê²°ë¡ : Sparkê°€ PostgreSQLë³´ë‹¤ {improvement:.1f}% ë” ë¹ ë¦„")
        print("   â†’ ë¶„ì‚° ì²˜ë¦¬ì™€ ì¸ë©”ëª¨ë¦¬ ì—°ì‚°ì˜ íš¨ê³¼")
        print("   â†’ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ë” í° ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€")
    else:
        slowdown = (sql_qps / spark_qps - 1) * 100
        print(f"âš ï¸ ê²°ë¡ : PostgreSQLì´ Sparkë³´ë‹¤ {slowdown:.1f}% ë” ë¹ ë¦„")
        print("   â†’ ì†Œê·œëª¨ ë°ì´í„°ì—ì„œëŠ” Spark ì˜¤ë²„í—¤ë“œê°€ ìˆì„ ìˆ˜ ìˆìŒ")
        print("   â†’ ë°ì´í„° ê·œëª¨ê°€ í´ìˆ˜ë¡ Sparkì˜ ì¥ì ì´ ë“œëŸ¬ë‚¨")
    
    # HTML ë¦¬í¬íŠ¸ ìƒì„±
    print()
    results = [sql_result, spark_result]
    html_path = generate_comparison_html(results, "spark_comparison_report")
    print(f"\nğŸ“„ HTML ë¦¬í¬íŠ¸: {html_path}")
    
    return results


if __name__ == "__main__":
    compare_results()
