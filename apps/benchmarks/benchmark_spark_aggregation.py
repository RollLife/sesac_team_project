"""
Spark ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬

Spark SQLì„ ì‚¬ìš©í•œ ë¶„ì‚° ì§‘ê³„ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
PostgreSQL ì§‘ê³„ì™€ ë¹„êµí•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.

ì£¼ì˜: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Spark í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
from datetime import datetime

# Spark ê´€ë ¨ import
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, count, sum as spark_sum, avg, date_trunc, date_format
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    print("âš ï¸ PySparkê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Spark í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")

# ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
NUM_ITERATIONS = int(os.environ.get('BENCHMARK_ITERATIONS', 10))
POSTGRES_URL = os.environ.get('POSTGRES_URL', 'jdbc:postgresql://postgres:5432/sesac_db')
POSTGRES_USER = os.environ.get('POSTGRES_USER', 'postgres')
POSTGRES_PASSWORD = os.environ.get('POSTGRES_PASSWORD', 'password')


def run_spark_benchmark():
    """Spark ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    if not SPARK_AVAILABLE:
        print("âŒ Sparkë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Spark SQL ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ë°˜ë³µ íšŸìˆ˜: {NUM_ITERATIONS:>10}íšŒ                                   â•‘
â•‘  JDBC URL: {POSTGRES_URL[:40]:<40}     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # Spark ì„¸ì…˜ ìƒì„±
    print("ğŸš€ Spark ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
    spark = SparkSession.builder \
        .appName("SparkBenchmark") \
        .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    try:
        orders_df = spark.read \
            .format("jdbc") \
            .option("url", POSTGRES_URL) \
            .option("dbtable", "orders") \
            .option("user", POSTGRES_USER) \
            .option("password", POSTGRES_PASSWORD) \
            .option("driver", "org.postgresql.Driver") \
            .load()
        
        products_df = spark.read \
            .format("jdbc") \
            .option("url", POSTGRES_URL) \
            .option("dbtable", "products") \
            .option("user", POSTGRES_USER) \
            .option("password", POSTGRES_PASSWORD) \
            .option("driver", "org.postgresql.Driver") \
            .load()
        
        users_df = spark.read \
            .format("jdbc") \
            .option("url", POSTGRES_URL) \
            .option("dbtable", "users") \
            .option("user", POSTGRES_USER) \
            .option("password", POSTGRES_PASSWORD) \
            .option("driver", "org.postgresql.Driver") \
            .load()
        
        # ìºì‹± (ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´)
        orders_df.cache()
        products_df.cache()
        users_df.cache()
        
        # ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        order_count = orders_df.count()
        print(f"ğŸ“Š orders í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜: {order_count:,}ê±´")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        spark.stop()
        return None

    print(f"\nğŸš€ Spark ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    print("-" * 60)
    
    total_start = time.perf_counter()
    all_latencies = []
    query_results = {}
    
    for iteration in range(NUM_ITERATIONS):
        iteration_start = time.perf_counter()
        
        # 1. ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ
        q1_start = time.perf_counter()
        category_stats = orders_df.join(products_df, orders_df.product_id == products_df.id) \
            .groupBy("category") \
            .agg(
                count("*").alias("order_count"),
                spark_sum("total_amount").alias("total_revenue"),
                avg("total_amount").alias("avg_order_value")
            ) \
            .orderBy(col("total_revenue").desc())
        category_result = category_stats.collect()
        q1_time = (time.perf_counter() - q1_start) * 1000
        all_latencies.append(q1_time)
        if 'category_revenue' not in query_results:
            query_results['category_revenue'] = {'latencies': [], 'row_count': len(category_result)}
        query_results['category_revenue']['latencies'].append(q1_time)
        
        # 2. ê²°ì œìˆ˜ë‹¨ë³„ í†µê³„
        q2_start = time.perf_counter()
        payment_stats = orders_df \
            .groupBy("payment_method") \
            .agg(
                count("*").alias("count"),
                spark_sum("total_amount").alias("total_revenue"),
                avg("total_amount").alias("avg_amount")
            ) \
            .orderBy(col("count").desc())
        payment_result = payment_stats.collect()
        q2_time = (time.perf_counter() - q2_start) * 1000
        all_latencies.append(q2_time)
        if 'payment_stats' not in query_results:
            query_results['payment_stats'] = {'latencies': [], 'row_count': len(payment_result)}
        query_results['payment_stats']['latencies'].append(q2_time)
        
        # 3. ì‹œê°„ë³„ ì£¼ë¬¸
        q3_start = time.perf_counter()
        hourly_orders = orders_df \
            .withColumn("hour", date_trunc("hour", "created_at")) \
            .groupBy("hour") \
            .agg(
                count("*").alias("order_count"),
                spark_sum("total_amount").alias("revenue")
            ) \
            .orderBy(col("hour").desc()) \
            .limit(24)
        hourly_result = hourly_orders.collect()
        q3_time = (time.perf_counter() - q3_start) * 1000
        all_latencies.append(q3_time)
        if 'hourly_orders' not in query_results:
            query_results['hourly_orders'] = {'latencies': [], 'row_count': len(hourly_result)}
        query_results['hourly_orders']['latencies'].append(q3_time)
        
        # 4. ìœ ì € ë“±ê¸‰ë³„ í†µê³„
        q4_start = time.perf_counter()
        user_stats = orders_df.join(users_df, orders_df.user_id == users_df.id) \
            .groupBy("grade") \
            .agg(
                count("*").alias("total_orders"),
                spark_sum("total_amount").alias("total_spent"),
                avg("total_amount").alias("avg_order")
            ) \
            .orderBy(col("total_spent").desc())
        user_result = user_stats.collect()
        q4_time = (time.perf_counter() - q4_start) * 1000
        all_latencies.append(q4_time)
        if 'user_stats' not in query_results:
            query_results['user_stats'] = {'latencies': [], 'row_count': len(user_result)}
        query_results['user_stats']['latencies'].append(q4_time)
        
        # 5. Top 20 ìƒí’ˆ
        q5_start = time.perf_counter()
        top_products = orders_df.join(products_df, orders_df.product_id == products_df.id) \
            .groupBy("name", "category") \
            .agg(
                count("*").alias("order_count"),
                spark_sum("total_amount").alias("total_revenue")
            ) \
            .orderBy(col("order_count").desc()) \
            .limit(20)
        top_result = top_products.collect()
        q5_time = (time.perf_counter() - q5_start) * 1000
        all_latencies.append(q5_time)
        if 'top_products' not in query_results:
            query_results['top_products'] = {'latencies': [], 'row_count': len(top_result)}
        query_results['top_products']['latencies'].append(q5_time)
        
        # 6. ì¼ë³„ íŠ¸ë Œë“œ
        q6_start = time.perf_counter()
        daily_trend = orders_df \
            .withColumn("date", date_format("created_at", "yyyy-MM-dd")) \
            .groupBy("date") \
            .agg(
                count("*").alias("order_count"),
                spark_sum("total_amount").alias("daily_revenue")
            ) \
            .orderBy(col("date").desc()) \
            .limit(30)
        daily_result = daily_trend.collect()
        q6_time = (time.perf_counter() - q6_start) * 1000
        all_latencies.append(q6_time)
        if 'daily_trend' not in query_results:
            query_results['daily_trend'] = {'latencies': [], 'row_count': len(daily_result)}
        query_results['daily_trend']['latencies'].append(q6_time)
        
        iteration_time = (time.perf_counter() - iteration_start) * 1000
        
        # ì§„í–‰ë¥  ì¶œë ¥
        if (iteration + 1) % max(1, NUM_ITERATIONS // 5) == 0:
            progress = (iteration + 1) / NUM_ITERATIONS * 100
            print(f"  â³ {progress:.0f}% ì™„ë£Œ | ë°˜ë³µ {iteration + 1}/{NUM_ITERATIONS} | {iteration_time:.1f}ms")

    total_duration = time.perf_counter() - total_start

    # ì¿¼ë¦¬ë³„ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„")
    print(f"{'='*60}")
    print(f"{'ì¿¼ë¦¬ëª…':<20} {'ê²°ê³¼í–‰':<8} {'í‰ê· (ms)':<10} {'ìµœì†Œ(ms)':<10} {'ìµœëŒ€(ms)':<10}")
    print("-" * 60)
    
    for query_name, data in query_results.items():
        latencies = data['latencies']
        if latencies:
            avg_lat = sum(latencies) / len(latencies)
            min_lat = min(latencies)
            max_lat = max(latencies)
            print(f"{query_name:<20} {data['row_count']:<8} {avg_lat:<10.2f} {min_lat:<10.2f} {max_lat:<10.2f}")

    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ benchmark_common ì‚¬ìš©
    # (Spark í™˜ê²½ì—ì„œëŠ” ì§ì ‘ JSON ì €ì¥)
    result_data = {
        "test_name": "Spark SQL ì§‘ê³„ ì¿¼ë¦¬",
        "mode": "spark",
        "total_records": len(query_results) * NUM_ITERATIONS,
        "duration_seconds": total_duration,
        "records_per_second": (len(query_results) * NUM_ITERATIONS) / total_duration if total_duration > 0 else 0,
        "avg_latency_ms": sum(all_latencies) / len(all_latencies) if all_latencies else 0,
        "min_latency_ms": min(all_latencies) if all_latencies else 0,
        "max_latency_ms": max(all_latencies) if all_latencies else 0,
        "success_count": len(query_results) * NUM_ITERATIONS,
        "failure_count": 0,
        "timestamp": datetime.now().isoformat(),
        "extra_info": {
            "query_count": len(query_results),
            "iterations": NUM_ITERATIONS,
            "total_orders": order_count,
            "query_stats": {
                name: {
                    'avg_ms': sum(data['latencies']) / len(data['latencies']) if data['latencies'] else 0,
                    'row_count': data['row_count']
                }
                for name, data in query_results.items()
            }
        }
    }
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    import json
    result_dir = "/app/benchmark_results"
    os.makedirs(result_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"spark_benchmark_spark_{timestamp}.json")
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {result_file}")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  í…ŒìŠ¤íŠ¸: Spark SQL ì§‘ê³„ ì¿¼ë¦¬                                  â•‘
â•‘  ëª¨ë“œ: SPARK                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ì´ ì¿¼ë¦¬: {len(query_results) * NUM_ITERATIONS:>10,}ê°œ                              â•‘
â•‘  ì†Œìš” ì‹œê°„: {total_duration:>10.2f}ì´ˆ                             â•‘
â•‘  QPS:       {result_data['records_per_second']:>10.2f} queries/sec                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  í‰ê·  ì§€ì—°: {result_data['avg_latency_ms']:>10.2f}ms                               â•‘
â•‘  ìµœì†Œ ì§€ì—°: {result_data['min_latency_ms']:>10.2f}ms                               â•‘
â•‘  ìµœëŒ€ ì§€ì—°: {result_data['max_latency_ms']:>10.2f}ms                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    spark.stop()
    return result_data


if __name__ == "__main__":
    result = run_spark_benchmark()
    if result:
        print("\nâœ… Spark ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    else:
        print("\nâŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨")
        sys.exit(1)
